{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Laptops data from BestBuy Canada website\n",
    "\n",
    "**Updated for Selenium-based scraping with modular architecture**\n",
    "This is an example of data scraping from BestBuy Canada website. This notebook demonstrates how to use the refactored webscraping project with Selenium WebDriver to handle JavaScript-rendered content.\n",
    "\n",
    "This exercise helps in deciding what computer to buy for data science projects by analyzing laptop data from BestBuy. We use Selenium WebDriver to interact with the website and automatically collect data about laptops from the dynamic React-based website.\n",
    "\n",
    "After getting the CSV file, we clean it, examine descriptive statistics, and create visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Project Structure\n",
    "\n",
    "The project has been reorganized into:\n",
    "- **src/** - All Python modules (config, scraper_selenium, data_cleaner, visualizer, logger)\n",
    "- **data/** - CSV data files\n",
    "- **notebooks/** - This tutorial notebook\n",
    "- **docs/** - Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import from src/\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# External library imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project module imports\n",
    "from src.logger import logger\n",
    "from src.config import get_config, build_url\n",
    "from src.scraper_selenium import BestBuySeleniumScraper, scrape_all_laptops\n",
    "from src.data_cleaner import save_data, load_and_process_data, clean_price, clean_votes\n",
    "from src.visualizer import visualize_data, create_histograms, create_boxplots\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Logger\n",
    "\n",
    "The project now uses **Loguru** for professional logging instead of print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate logging\n",
    "logger.info(\"Starting laptop data analysis tutorial with Selenium\")\n",
    "logger.debug(\"This is a debug message\")\n",
    "logger.warning(\"This is a warning message\")\n",
    "logger.success(\"Logger is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load the scraping configuration from the config module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration\n",
    "config = get_config()\n",
    "logger.info(f\"Configuration loaded: {len(config['pages'])} pages, {len(config['ram_sizes'])} RAM sizes\")\n",
    "print(f\"Pages to scrape: {config['pages'][:5]}...\")  # Show first 5\n",
    "print(f\"RAM sizes: {config['ram_sizes']}\")\n",
    "print(f\"Output file: {config['output_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Selenium?\n",
    "\n",
    "Best Buy Canada's website is built with React and loads content dynamically using JavaScript. Traditional HTTP requests (like `requests` library) only get the initial HTML, which doesn't contain the product data. We need Selenium WebDriver to:\n",
    "\n",
    "1. Open a real browser (Chrome)\n",
    "2. Wait for JavaScript to execute\n",
    "3. Let React render the product components\n",
    "4. Extract the fully-rendered HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a test URL\n",
    "test_url = build_url('1', '8')\n",
    "logger.info(f\"Test URL: {test_url}\")\n",
    "\n",
    "# Create a scraper instance (use headless=False to see the browser in action)\n",
    "scraper = BestBuySeleniumScraper(config, headless=True)\n",
    "\n",
    "try:\n",
    "    # Setup the driver\n",
    "    if scraper.setup_driver():\n",
    "        logger.success(\"WebDriver initialized successfully!\")\n",
    "        \n",
    "        # Load the page\n",
    "        scraper.driver.get(test_url)\n",
    "        logger.info(\"Page loaded, waiting for products...\")\n",
    "        \n",
    "        # Wait for products to load\n",
    "        if scraper.wait_for_products(timeout=15):\n",
    "            logger.success(\"Products detected on page!\")\n",
    "            \n",
    "            # Get containers\n",
    "            from bs4 import BeautifulSoup\n",
    "            page_source = scraper.driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            containers = soup.find_all('div', {'itemtype': lambda x: x and 'Product' in x})\n",
    "            \n",
    "            logger.info(f\"Found {len(containers)} product containers\")\n",
    "            \n",
    "            # Try to extract first product\n",
    "            if containers:\n",
    "                first_product = scraper.extract_laptop_data(containers[0])\n",
    "                if first_product:\n",
    "                    logger.success(\"Successfully extracted first product:\")\n",
    "                    for key, value in first_product.items():\n",
    "                        print(f\"  {key}: {value}\")\n",
    "                else:\n",
    "                    logger.warning(\"Could not extract data from first container\")\n",
    "        else:\n",
    "            logger.error(\"Products did not load in time\")\n",
    "            \n",
    "finally:\n",
    "    # Always close the driver\n",
    "    scraper.close_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data from First Laptop\n",
    "\n",
    "Let's examine the first laptop container to understand the data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Run Full Scraper\n",
    "\n",
    "**Warning:** This will make many requests and may take a long time. Consider using existing data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN FULL SCRAPING (can take 20-30 minutes)\n",
    "# logger.info(\"Starting full Selenium web scraping...\")\n",
    "# data = scrape_all_laptops(config, build_url)\n",
    "# logger.success(f\"Scraping complete! Collected {len(data['names'])} laptops\")\n",
    "#\n",
    "# # Save the data\n",
    "# output_path = config['output_file']\n",
    "# df = save_data(data, output_path)\n",
    "# logger.success(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Load Existing Data\n",
    "\n",
    "Let's use the existing scraped data from the data/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data\n",
    "# Try the 2025 data first, fall back to 2019 data if not available\n",
    "import os\n",
    "data_files = [\n",
    "    '../data/laptops_bestbuy_2025.csv',\n",
    "    '../data/laptops_rating2019.csv',\n",
    "    '../data/laptops_rating.csv'\n",
    "]\n",
    "\n",
    "df_raw = None\n",
    "data_file = None\n",
    "\n",
    "for file in data_files:\n",
    "    if os.path.exists(file):\n",
    "        data_file = file\n",
    "        logger.info(f\"Loading data from {data_file}\")\n",
    "        try:\n",
    "            df_raw = pd.read_csv(data_file)\n",
    "            logger.success(f\"Data loaded successfully! Shape: {df_raw.shape}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "if df_raw is None:\n",
    "    logger.error(\"No data file found! Please run the scraper first or check data directory.\")\n",
    "else:\n",
    "    display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Use the data_cleaner module to process the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Clean a single price\n",
    "sample_price = '$1,234.56'\n",
    "cleaned_price = clean_price(sample_price)\n",
    "logger.info(f\"Price cleaning example: '{sample_price}' -> {cleaned_price}\")\n",
    "\n",
    "# Example: Clean a vote count\n",
    "sample_vote = '(123)'\n",
    "cleaned_vote = clean_votes(sample_vote)\n",
    "logger.info(f\"Vote cleaning example: '{sample_vote}' -> {cleaned_vote}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the entire dataframe\n",
    "logger.info(\"Cleaning dataframe...\")\n",
    "df_clean = load_and_process_data(data_file)\n",
    "logger.success(\"Data cleaning complete!\")\n",
    "\n",
    "# Display cleaned data info\n",
    "print(\"\\nCleaned Data Info:\")\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Computing descriptive statistics...\")\n",
    "stats = df_clean[['prices', 'ratings', 'votes']].describe()\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Interesting Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find laptop with most votes\n",
    "max_votes_idx = df_clean['votes'].idxmax()\n",
    "most_voted = df_clean.loc[max_votes_idx]\n",
    "logger.info(f\"Most voted laptop: {most_voted['laptops']} with {most_voted['votes']} votes\")\n",
    "print(f\"\\nMost Voted Laptop:\")\n",
    "print(f\"  Name: {most_voted['laptops']}\")\n",
    "print(f\"  Price: ${most_voted['prices']:.2f}\")\n",
    "print(f\"  Rating: {most_voted['ratings']}%\")\n",
    "print(f\"  Votes: {most_voted['votes']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find laptops with perfect rating (100%)\n",
    "perfect_ratings = df_clean[df_clean['ratings'] == 100.0]\n",
    "logger.info(f\"Found {len(perfect_ratings)} laptops with 100% rating\")\n",
    "if len(perfect_ratings) > 0:\n",
    "    print(\"\\nLaptops with 100% rating:\")\n",
    "    print(perfect_ratings[['laptops', 'prices', 'votes']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Use the visualizer module to create charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms\n",
    "logger.info(\"Creating histograms...\")\n",
    "create_histograms(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots\n",
    "logger.info(\"Creating boxplots...\")\n",
    "create_boxplots(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Visualization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the complete visualization function\n",
    "logger.info(\"Running complete visualization pipeline...\")\n",
    "visualize_data(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_outliers_iqr(df, column):\n",
    "    \"\"\"\n",
    "    Removes outliers from a specified column of a DataFrame using the 1.5 * IQR rule.\n",
    "    Returns a new DataFrame with outliers filtered out.\n",
    "    \"\"\"\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    \n",
    "    # Calculate the Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define bounds for outlier detection (1.5 * IQR)\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter the DataFrame: keep rows where the column value is within the bounds\n",
    "    mask = (df[column] >= lower_bound) & (df[column] <= upper_bound)\n",
    "    return df[mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Application Logic ---\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df_cleaned = df_clean.copy() \n",
    "\n",
    "# List of columns to check for outliers\n",
    "columns_to_check = ['prices', 'ratings', 'votes']\n",
    "\n",
    "for col in columns_to_check:\n",
    "    # Print the bounds for logging/checking (optional)\n",
    "    Q1 = df_cleaned[col].quantile(0.25)\n",
    "    Q3 = df_cleaned[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    print(f\"Processing column: {col}\")\n",
    "    print(f\"  Bounds: ({lower_bound:.2f}, {upper_bound:.2f})\")\n",
    "    \n",
    "# Apply the filtering function and update the DataFrame\n",
    "df_cleaned = remove_outliers_iqr(df_cleaned, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the complete visualization function\n",
    "logger.info(\"Running complete visualization pipeline...\")\n",
    "visualize_data(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Analysis\n",
    "\n",
    "Perform additional custom analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution by rating category\n",
    "logger.info(\"Analyzing price by rating category...\")\n",
    "\n",
    "# Categorize ratings\n",
    "df_clean['rating_category'] = pd.cut(\n",
    "    df_clean['ratings'], \n",
    "    bins=[0, 70, 85, 95, 100],\n",
    "    labels=['Low (0-70)', 'Medium (70-85)', 'High (85-95)', 'Excellent (95-100)']\n",
    ")\n",
    "\n",
    "# Group by category and show mean price\n",
    "price_by_rating = df_clean.groupby('rating_category')['prices'].agg(['mean', 'median', 'count'])\n",
    "print(\"\\nPrice Statistics by Rating Category:\")\n",
    "print(price_by_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price vs rating category\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_clean.boxplot(column='prices', by='rating_category', figsize=(12, 6))\n",
    "plt.title('Price Distribution by Rating Category')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "plt.ylabel('Price ($)')\n",
    "plt.xlabel('Rating Category')\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Modular Code Structure**: Using functions from `src/` modules\n",
    "2. **Loguru Logging**: Professional logging instead of print statements\n",
    "3. **Data Pipeline**: Configuration → Scraping → Cleaning → Visualization\n",
    "4. **Data Analysis**: Descriptive statistics and custom insights\n",
    "5. **Visualization**: Histograms, boxplots, and custom charts\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore different RAM sizes and price ranges\n",
    "- Add more sophisticated analysis (correlation, regression)\n",
    "- Create a dashboard for interactive exploration\n",
    "- Schedule regular scraping to track price trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.success(\"Tutorial complete! You've learned how to use Selenium-based webscraping with the updated modules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

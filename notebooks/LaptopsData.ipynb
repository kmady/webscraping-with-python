{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Laptops data from BestBuy Canada website\n",
    "\n",
    "**Updated for modular architecture with Loguru logging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of data scraping from BestBuy Canada website. This notebook demonstrates how to use the refactored webscraping project with modular code structure.\n",
    "\n",
    "This exercise helps in deciding what computer to buy for data science projects by analyzing laptop data from BestBuy. We make requests to the website and use BeautifulSoup to parse HTML and automatically collect data about computers.\n",
    "\n",
    "After getting the CSV file, we clean it, examine descriptive statistics, and create visualizations.\n",
    "\n",
    "## New Project Structure\n",
    "\n",
    "The project has been reorganized into:\n",
    "- **src/** - All Python modules (config, scraper, data_cleaner, visualizer, logger)\n",
    "- **data/** - CSV data files\n",
    "- **notebooks/** - This tutorial notebook\n",
    "- **docs/** - Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import from src/\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# External library imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "# Project module imports\n",
    "from src.logger import logger\n",
    "from src.config import get_config, build_url\n",
    "from src.scraper import scrape_all_laptops, extract_laptop_data\n",
    "from src.data_cleaner import save_data, load_and_process_data, clean_price, clean_votes\n",
    "from src.visualizer import visualize_data, create_histograms, create_boxplots\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Logger\n",
    "\n",
    "The project now uses **Loguru** for professional logging instead of print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-09 14:12:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mStarting laptop data analysis tutorial\u001b[0m\n",
      "\u001b[32m2025-12-09 14:12:33\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[33m\u001b[1mThis is a warning message\u001b[0m\n",
      "\u001b[32m2025-12-09 14:12:33\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[32m\u001b[1mLogger is working correctly!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate logging\n",
    "logger.info(\"Starting laptop data analysis tutorial\")\n",
    "logger.debug(\"This is a debug message\")\n",
    "logger.warning(\"This is a warning message\")\n",
    "logger.success(\"Logger is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load the scraping configuration from the config module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-09 14:12:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mConfiguration loaded: 19 pages, 7 RAM sizes\u001b[0m\n",
      "Pages to scrape: ['1', '2', '3', '4', '5']...\n",
      "RAM sizes: ['2', '4', '8', '12', '16', '32', '64']\n",
      "Output file: laptops_rating2019.csv\n"
     ]
    }
   ],
   "source": [
    "# Get configuration\n",
    "config = get_config()\n",
    "logger.info(f\"Configuration loaded: {len(config['pages'])} pages, {len(config['ram_sizes'])} RAM sizes\")\n",
    "print(f\"Pages to scrape: {config['pages'][:5]}...\")  # Show first 5\n",
    "print(f\"RAM sizes: {config['ram_sizes']}\")\n",
    "print(f\"Output file: {config['output_file']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single URL Request\n",
    "\n",
    "Let's test a single request to understand the HTML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-09 14:13:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mTest URL: https://www.bestbuy.ca/en-ca/category/windows-laptops/36711?page=1&path=category%3AComputers%2B%2526%2BTablets%3Bcategory%3ALaptops%2B%2526%2BMacBooks%3Bcategory%3AWindows%2BLaptops%3Bcustom0ramsize%3A8\u001b[0m\n",
      "\u001b[32m2025-12-09 14:13:12\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[31m\u001b[1mError fetching URL: HTTPSConnectionPool(host='www.bestbuy.ca', port=443): Read timed out. (read timeout=10)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Build a test URL\n",
    "test_url = build_url('1', '8')\n",
    "logger.info(f\"Test URL: {test_url}\")\n",
    "\n",
    "try:\n",
    "    response = get(test_url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    logger.success(f\"Successfully fetched URL. Status code: {response.status_code}\")\n",
    "    logger.info(f\"Response length: {len(response.text)} characters\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error fetching URL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "logger.info(f\"HTML parsed. Type: {type(html_soup)}\")\n",
    "\n",
    "# Find laptop containers\n",
    "laptop_containers = html_soup.find_all('div', class_='item-inner clearfix')\n",
    "logger.info(f\"Found {len(laptop_containers)} laptop containers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data from First Laptop\n",
    "\n",
    "Let's examine the first laptop container to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if laptop_containers:\n",
    "    first_laptop = laptop_containers[0]\n",
    "    \n",
    "    # Check if it has a rating\n",
    "    if first_laptop.find('div', class_='rating-stars-yellow') is not None:\n",
    "        laptop_data = extract_laptop_data(first_laptop)\n",
    "        logger.info(\"Extracted data from first laptop:\")\n",
    "        for key, value in laptop_data.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        logger.warning(\"First laptop has no rating\")\n",
    "else:\n",
    "    logger.error(\"No laptop containers found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Run Full Scraper\n",
    "\n",
    "**Warning:** This will make many requests and may take a long time. Consider using existing data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN FULL SCRAPING (can take 20+ minutes)\n",
    "# logger.info(\"Starting full web scraping...\")\n",
    "# data = scrape_all_laptops(config, build_url)\n",
    "# logger.success(f\"Scraping complete! Collected {len(data['names'])} laptops\")\n",
    "#\n",
    "# # Save the data\n",
    "# output_path = f\"../data/{config['output_file']}\"\n",
    "# save_data(data, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Load Existing Data\n",
    "\n",
    "Let's use the existing scraped data from the data/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data\n",
    "data_file = '../data/laptops_rating2019.csv'\n",
    "logger.info(f\"Loading data from {data_file}\")\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv(data_file)\n",
    "    logger.success(f\"Data loaded successfully! Shape: {df_raw.shape}\")\n",
    "    df_raw.head()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Use the data_cleaner module to process the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Clean a single price\n",
    "sample_price = '$1,234.56'\n",
    "cleaned_price = clean_price(sample_price)\n",
    "logger.info(f\"Price cleaning example: '{sample_price}' -> {cleaned_price}\")\n",
    "\n",
    "# Example: Clean a vote count\n",
    "sample_vote = '(123)'\n",
    "cleaned_vote = clean_votes(sample_vote)\n",
    "logger.info(f\"Vote cleaning example: '{sample_vote}' -> {cleaned_vote}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the entire dataframe\n",
    "logger.info(\"Cleaning dataframe...\")\n",
    "df_clean = load_and_process_data(data_file)\n",
    "logger.success(\"Data cleaning complete!\")\n",
    "\n",
    "# Display cleaned data info\n",
    "print(\"\\nCleaned Data Info:\")\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Computing descriptive statistics...\")\n",
    "stats = df_clean[['prices', 'ratings', 'votes']].describe()\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Interesting Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find laptop with most votes\n",
    "max_votes_idx = df_clean['votes'].idxmax()\n",
    "most_voted = df_clean.loc[max_votes_idx]\n",
    "logger.info(f\"Most voted laptop: {most_voted['laptops']} with {most_voted['votes']} votes\")\n",
    "print(f\"\\nMost Voted Laptop:\")\n",
    "print(f\"  Name: {most_voted['laptops']}\")\n",
    "print(f\"  Price: ${most_voted['prices']:.2f}\")\n",
    "print(f\"  Rating: {most_voted['ratings']}%\")\n",
    "print(f\"  Votes: {most_voted['votes']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find laptops with perfect rating (100%)\n",
    "perfect_ratings = df_clean[df_clean['ratings'] == 100.0]\n",
    "logger.info(f\"Found {len(perfect_ratings)} laptops with 100% rating\")\n",
    "if len(perfect_ratings) > 0:\n",
    "    print(\"\\nLaptops with 100% rating:\")\n",
    "    print(perfect_ratings[['laptops', 'prices', 'votes']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Use the visualizer module to create charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms\n",
    "logger.info(\"Creating histograms...\")\n",
    "create_histograms(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots\n",
    "logger.info(\"Creating boxplots...\")\n",
    "create_boxplots(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Visualization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the complete visualization function\n",
    "logger.info(\"Running complete visualization pipeline...\")\n",
    "visualize_data(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Analysis\n",
    "\n",
    "Perform additional custom analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution by rating category\n",
    "logger.info(\"Analyzing price by rating category...\")\n",
    "\n",
    "# Categorize ratings\n",
    "df_clean['rating_category'] = pd.cut(\n",
    "    df_clean['ratings'], \n",
    "    bins=[0, 70, 85, 95, 100],\n",
    "    labels=['Low (0-70)', 'Medium (70-85)', 'High (85-95)', 'Excellent (95-100)']\n",
    ")\n",
    "\n",
    "# Group by category and show mean price\n",
    "price_by_rating = df_clean.groupby('rating_category')['prices'].agg(['mean', 'median', 'count'])\n",
    "print(\"\\nPrice Statistics by Rating Category:\")\n",
    "print(price_by_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price vs rating category\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_clean.boxplot(column='prices', by='rating_category', figsize=(12, 6))\n",
    "plt.title('Price Distribution by Rating Category')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "plt.ylabel('Price ($)')\n",
    "plt.xlabel('Rating Category')\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Modular Code Structure**: Using functions from `src/` modules\n",
    "2. **Loguru Logging**: Professional logging instead of print statements\n",
    "3. **Data Pipeline**: Configuration → Scraping → Cleaning → Visualization\n",
    "4. **Data Analysis**: Descriptive statistics and custom insights\n",
    "5. **Visualization**: Histograms, boxplots, and custom charts\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore different RAM sizes and price ranges\n",
    "- Add more sophisticated analysis (correlation, regression)\n",
    "- Create a dashboard for interactive exploration\n",
    "- Schedule regular scraping to track price trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.success(\"Tutorial complete! You've learned how to use the webscraping project modules.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
